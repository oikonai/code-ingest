openapi: 3.0.3
info:
  title: I2P Modal Services API
  description: |
    Production-ready AI services deployed on Modal for I2P (Issue to Prompt) system.

    ## Services

    - **DeepSeek OCR** - Document OCR with DeepSeek-OCR (3B params)
    - **PaddleOCR-VL** - Lightweight OCR with PaddleOCR-VL (0.9B params) via OpenAI API
    - **Qwen Embedding** - Text embeddings with Qwen3-Embedding-8B via TEI
    - **NuExtract** - Structured information extraction with NuExtract-2.0-8B (8B params)

    ## Base URLs

    Production endpoints are dynamically generated by Modal:
    - DeepSeek OCR: `https://your-org--ocr-process.modal.run`
    - PaddleOCR: `https://your-org--pocr-ocr-chat.modal.run`
    - Embedding: `https://your-org--embed.modal.run`
    - Extraction: `https://your-org--extract.modal.run`

  version: 1.0.0
  contact:
    name: I2P Development Team
    url: https://github.com/your-org/code-ingest

servers:
  - url: https://your-org--ocr-process.modal.run
    description: DeepSeek OCR Service (Production - vLLM Backend)
  - url: https://your-org--pocr-ocr-chat.modal.run
    description: PaddleOCR-VL Service (Production - vLLM/OpenAI API)
  - url: https://your-org--embed.modal.run
    description: Qwen Embedding Service (Production)
  - url: https://your-org--extract.modal.run
    description: NuExtract Service (Production)

tags:
  - name: OCR
    description: Document optical character recognition
  - name: Embedding
    description: Text embedding generation
  - name: Extraction
    description: Structured information extraction

paths:
  # ==================== DeepSeek OCR Service ====================

  /:
    post:
      tags:
        - OCR
      summary: Process image or PDF document
      description: |
        Extract text and markdown from images or PDF documents using DeepSeek-OCR.

        **Model Specifications:**
        - Model: deepseek-ai/DeepSeek-OCR
        - Parameters: 3 billion (3B)
        - Architecture: Vision Transformer with OCR head
        - Precision: bfloat16 (BF16)
        - Backend: vLLM with continuous batching
        - Hardware: NVIDIA A100-80GB GPU

        **Processing Modes:**
        - **Single Page**: Specify `page_num` for specific PDF page (sequential processing)
        - **Multi-Page**: Omit `page_num` to process all pages (parallel batch processing)

        **Performance:**
        - Throughput: ~2.9 pages/sec (180 pages in ~63s)
        - Single page: ~350ms per page
        - Continuous batching: Up to 100 concurrent requests

        **Input Sources:**
        You must provide ONE of:
        - `image_url` - URL to an image
        - `image_base64` - Base64-encoded image
        - `pdf_url` - URL to a PDF
        - `pdf_base64` - Base64-encoded PDF

      operationId: processImage
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/OCRRequest'
            examples:
              singlePagePDF:
                summary: Extract specific PDF page
                value:
                  pdf_base64: "JVBERi0xLjQKJeLjz9M..."
                  page_num: 1
                  prompt: "<image>\n<|grounding|>Convert to markdown"
                  base_size: 1024
                  image_size: 640

              multiPagePDF:
                summary: Process entire PDF (parallel)
                value:
                  pdf_base64: "JVBERi0xLjQKJeLjz9M..."
                  prompt: "<image>\n<|grounding|>Convert the document to markdown"

              imageURL:
                summary: Process image from URL
                value:
                  image_url: "https://example.com/document.png"
                  prompt: "<image>\n<|grounding|>Extract text"

      responses:
        '200':
          description: OCR processing successful
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/OCRResponse'
              examples:
                success:
                  summary: Successful extraction
                  value:
                    text: |
                      <|ref|>title<|/ref|><|det|>[[52, 45, 940, 550]]<|/det|>
                      # The Longer Catechism

                      <|ref|>text<|/ref|><|det|>[[52, 720, 760, 975]]<|/det|>
                      Sample text content here...
                    model: "deepseek-ai/DeepSeek-OCR"
                    processing_time_ms: 350.0
                    pages_processed: 1
                    success: true
                    backend: "vllm-official-upstream"

                multiPageSuccess:
                  summary: Multi-page PDF processed
                  value:
                    text: "# Page 1 content...\n\n---PAGE BREAK---\n\n# Page 2 content..."
                    model: "deepseek-ai/DeepSeek-OCR"
                    processing_time_ms: 62832.7
                    pages_processed: 180
                    throughput_pages_per_sec: 2.86
                    success: true
                    backend: "vllm-official-upstream"

        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                noInput:
                  summary: Missing input
                  value:
                    error: "No input provided (need image_url, image_base64, pdf_url, or pdf_base64)"
                    success: false

                invalidPageNum:
                  summary: Invalid page number
                  value:
                    error: "Invalid page_num: 999 (PDF has 180 pages)"
                    success: false

        '500':
          description: Processing failed
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                processingError:
                  summary: Model processing error
                  value:
                    error: "Processing failed: CUDA out of memory"
                    traceback: "Traceback (most recent call last)..."
                    success: false

  /health:
    get:
      tags:
        - OCR
      summary: OCR service health check
      description: Check if the DeepSeek OCR service is healthy and ready
      operationId: ocrHealth
      responses:
        '200':
          description: Service is healthy
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "healthy"
                  model:
                    type: string
                    example: "deepseek-ai/DeepSeek-OCR"
                  parameters:
                    type: string
                    example: "3B"
                  backend:
                    type: string
                    example: "vllm-official-upstream"
                  max_concurrent:
                    type: integer
                    example: 100
                  gpu_utilization:
                    type: number
                    example: 0.9
                  expected_throughput:
                    type: string
                    example: "40-100 pages/sec"

  # ==================== PaddleOCR-VL Service (OpenAI API) ====================

  /v1/chat/completions:
    post:
      tags:
        - OCR
      summary: PaddleOCR-VL Chat Completions (OpenAI-compatible)
      description: |
        Perform OCR on images using PaddleOCR-VL through OpenAI-compatible chat completions API.

        **Model Specifications:**
        - Model: PaddlePaddle/PaddleOCR-VL
        - Parameters: 0.9 billion (0.9B)
        - Architecture: Vision-Language model optimized for OCR
        - Precision: bfloat16 (BF16)
        - Backend: vLLM serve with OpenAI-compatible API
        - Hardware: NVIDIA A100 GPU (40GB)

        **Capabilities:**
        - OCR (text extraction)
        - Table recognition
        - Formula recognition
        - Chart recognition

        **Performance:**
        - Throughput: ~1.8 pages/second (with 16 parallel workers)
        - Latency: 0.6-2.5s per page
        - Concurrent requests: Up to 100 per container
        - Accuracy: 99.16% character-level, 98.49% word-level

        **Usage:**
        Send images as base64-encoded data URLs in OpenAI chat format.
        Use the prompt "OCR:" to trigger text extraction.

      operationId: paddleOCRChatCompletion
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/PaddleOCRRequest'
            examples:
              singleImage:
                summary: OCR single image
                value:
                  model: "PaddlePaddle/PaddleOCR-VL"
                  messages:
                    - role: "user"
                      content:
                        - type: "image_url"
                          image_url:
                            url: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..."
                        - type: "text"
                          text: "OCR:"
                  temperature: 0.0
                  max_tokens: 4096

              pdfPage:
                summary: OCR PDF page (converted to PNG)
                value:
                  model: "PaddlePaddle/PaddleOCR-VL"
                  messages:
                    - role: "user"
                      content:
                        - type: "image_url"
                          image_url:
                            url: "data:image/png;base64,JVBERi0xLjQK..."
                        - type: "text"
                          text: "OCR:"
                  temperature: 0.0
                  max_tokens: 4096

      responses:
        '200':
          description: OCR processing successful
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PaddleOCRResponse'
              examples:
                success:
                  summary: Successful OCR extraction
                  value:
                    id: "chatcmpl-123"
                    object: "chat.completion"
                    created: 1677652288
                    model: "PaddlePaddle/PaddleOCR-VL"
                    choices:
                      - index: 0
                        message:
                          role: "assistant"
                          content: |
                            # Credit Agreement

                            This agreement is made between XPEL, INC. and Wells Fargo Bank...
                        finish_reason: "stop"
                    usage:
                      prompt_tokens: 512
                      completion_tokens: 1024
                      total_tokens: 1536

        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                invalidFormat:
                  summary: Invalid image format
                  value:
                    error: "cannot identify image file"

        '500':
          description: Processing failed
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                processingError:
                  summary: vLLM processing error
                  value:
                    error: "Model inference failed"

  /paddleocr-health:
    get:
      tags:
        - OCR
      summary: PaddleOCR service health check
      description: Check if the PaddleOCR-VL service is healthy and ready
      operationId: paddleOCRHealth
      responses:
        '200':
          description: Service is healthy
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "healthy"
                  model:
                    type: string
                    example: "PaddlePaddle/PaddleOCR-VL"
                  parameters:
                    type: string
                    example: "0.9B"
                  capabilities:
                    type: array
                    items:
                      type: string
                    example: ["ocr", "table", "formula", "chart"]
                  backend:
                    type: string
                    example: "vLLM-serve"
                  api:
                    type: string
                    example: "OpenAI-compatible"

  # ==================== Qwen Embedding Service ====================

  /embed:
    post:
      tags:
        - Embedding
      summary: Generate text embeddings
      description: |
        Generate high-quality embeddings using Qwen3-Embedding-8B via Text Embeddings Inference (TEI).

        **Model Specifications:**
        - Model: Qwen/Qwen3-Embedding-8B
        - Parameters: 8 billion (8B)
        - Architecture: Transformer encoder (embedding-optimized)
        - Output Dimensions: 8,192
        - Context Window: 32,768 tokens (auto-truncate enabled)
        - Precision: float16 (FP16)
        - Backend: HuggingFace TEI 1.7.2
        - Hardware: NVIDIA A100 GPU (80GB)

        **Performance:**
        - Batch size: Up to 100 texts per request
        - Concurrent requests: 512 max
        - Throughput: ~500-1000 texts/second (warm)

        **Features:**
        - Mean pooling
        - FP16 precision for speed
        - Automatic value validation (handles None/NaN)
        - Concurrent batch processing

      operationId: embed
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/EmbeddingRequest'
            examples:
              singleText:
                summary: Embed single text
                value:
                  texts: ["This is a sample document for embedding."]

              batchTexts:
                summary: Embed multiple texts
                value:
                  texts:
                    - "First document to embed"
                    - "Second document with different content"
                    - "Third document for semantic search"

      responses:
        '200':
          description: Embeddings generated successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/EmbeddingResponse'
              examples:
                success:
                  summary: Successful embedding
                  value:
                    results:
                      - embedding: [0.123, -0.456, 0.789, ...]
                        model: "Qwen/Qwen3-Embedding-8B"
                        dimensions: 8192
                        success: true
                        backend: "TEI"
                      - embedding: [0.234, -0.567, 0.890, ...]
                        model: "Qwen/Qwen3-Embedding-8B"
                        dimensions: 8192
                        success: true
                        backend: "TEI"
                    batch_size: 2
                    model: "qwen3-embedding-8b"
                    total_request_time_ms: 234.5
                    backend: "TEI"

        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                noTexts:
                  summary: Missing texts
                  value:
                    error: "No texts provided"

        '500':
          description: Embedding generation failed
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                teiError:
                  summary: TEI backend error
                  value:
                    error: "TEI returned HTTP 500"
                    details: "Internal Server Error"

  /info:
    get:
      tags:
        - Embedding
      summary: Get embedding model information
      description: Retrieve model specifications and configuration from TEI backend
      operationId: embeddingInfo
      responses:
        '200':
          description: Model information
          content:
            application/json:
              schema:
                type: object
                properties:
                  model_id:
                    type: string
                    example: "Qwen/Qwen3-Embedding-8B"
                  max_input_length:
                    type: integer
                    example: 32768
                  tokenizer_name:
                    type: string
                    example: "Qwen/Qwen3-Embedding-8B"
                  embedding_dimensions:
                    type: integer
                    example: 8192

  # ==================== NuExtract Service ====================

  /extract:
    post:
      tags:
        - Extraction
      summary: Extract structured information from text or images
      description: |
        Extract structured data from text or images using NuExtract-2.0-8B with custom JSON templates.

        **Model Specifications:**
        - Model: numind/NuExtract-2.0-8B
        - Architecture: Qwen2.5-VL (Vision-Language multimodal)
        - Parameters: 8 billion (8B)
        - Context Window: 8,192 tokens
        - Precision: bfloat16 (Brain Floating Point 16-bit)
        - Backend: vLLM 0.11.0 with multimodal support
        - Hardware: NVIDIA A100 GPU (80GB)

        **Performance:**
        - Latency: ~0.9-1.5s per extraction (warm)
        - Cold start: ~60-90s (first request)
        - Concurrent requests: Up to 10 per container
        - Max containers: 3

        **Input Modes:**
        - **Text-only**: Provide `text` parameter
        - **Image**: Provide `image_url` or `image_base64`
        - **Template**: Required - defines extraction schema

        **Use Cases:**
        - Issue/ticket parsing
        - Contract data extraction
        - Form data extraction
        - Structured document parsing

      operationId: extract
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ExtractionRequest'
            examples:
              textExtraction:
                summary: Extract from text
                value:
                  text: "Critical bug in the authentication module causing system crashes. Severity is high and affects the login component."
                  template:
                    issue_type: ""
                    severity: ""
                    affected_component: ""
                  temperature: 0.0

              imageExtraction:
                summary: Extract from image
                value:
                  image_url: "https://example.com/issue-screenshot.png"
                  template:
                    issue_type: ""
                    priority: ""
                    description: ""
                  temperature: 0.0

              complexTemplate:
                summary: Complex nested template
                value:
                  text: "Contract signed on 2024-01-15 between Acme Corp and Tech Inc for $500,000. Payment terms: Net 30. Deliverables include software development and maintenance."
                  template:
                    contract_date: ""
                    parties: []
                    amount: 0
                    payment_terms: ""
                    deliverables: []
                  temperature: 0.0

      responses:
        '200':
          description: Extraction successful
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExtractionResponse'
              examples:
                success:
                  summary: Successful extraction
                  value:
                    extracted_data:
                      issue_type: "Bug"
                      severity: "High"
                      affected_component: "Login component"
                    model: "numind/NuExtract-2.0-8B"
                    processing_time_ms: 396.27
                    template:
                      issue_type: ""
                      severity: ""
                      affected_component: ""
                    success: true
                    backend: "vLLM"

        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                noInput:
                  summary: Missing input
                  value:
                    error: "No text or image provided"
                    success: false

                noTemplate:
                  summary: Missing template
                  value:
                    error: "No template provided"
                    success: false

        '500':
          description: Extraction failed
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                processingError:
                  summary: Model processing error
                  value:
                    error: "Extraction failed: Model inference error"
                    success: false

  /nuextract-health:
    get:
      tags:
        - Extraction
      summary: NuExtract service health check
      description: Check if the NuExtract service is healthy and ready
      operationId: nuextractHealth
      responses:
        '200':
          description: Service is healthy
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "healthy"
                  model:
                    type: string
                    example: "numind/NuExtract-2.0-8B"
                  parameters:
                    type: string
                    example: "8B"
                  capabilities:
                    type: array
                    items:
                      type: string
                    example: ["text", "multimodal"]
                  backend:
                    type: string
                    example: "vLLM"

# ==================== Component Schemas ====================

components:
  schemas:
    # PaddleOCR Schemas (OpenAI-compatible)
    PaddleOCRRequest:
      type: object
      description: OpenAI-compatible chat completion request for PaddleOCR-VL
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: Model identifier (always PaddlePaddle/PaddleOCR-VL)
          example: "PaddlePaddle/PaddleOCR-VL"

        messages:
          type: array
          minItems: 1
          description: Array of message objects in OpenAI format
          items:
            type: object
            required:
              - role
              - content
            properties:
              role:
                type: string
                enum: [user, assistant, system]
                description: Message role
                example: "user"
              content:
                type: array
                description: Array of content parts (text and/or images)
                items:
                  oneOf:
                    - type: object
                      description: Text content
                      required:
                        - type
                        - text
                      properties:
                        type:
                          type: string
                          enum: [text]
                        text:
                          type: string
                          example: "OCR:"
                    - type: object
                      description: Image URL content
                      required:
                        - type
                        - image_url
                      properties:
                        type:
                          type: string
                          enum: [image_url]
                        image_url:
                          type: object
                          required:
                            - url
                          properties:
                            url:
                              type: string
                              format: uri
                              description: Image data URL (base64-encoded)
                              example: "data:image/png;base64,iVBORw0KGgo..."

        temperature:
          type: number
          format: float
          minimum: 0.0
          maximum: 1.0
          default: 0.0
          description: Sampling temperature (use 0.0 for deterministic OCR)
          example: 0.0

        max_tokens:
          type: integer
          minimum: 1
          maximum: 16384
          default: 4096
          description: Maximum tokens to generate
          example: 4096

    PaddleOCRResponse:
      type: object
      description: OpenAI-compatible chat completion response with OCR results
      required:
        - id
        - object
        - created
        - model
        - choices
        - usage
      properties:
        id:
          type: string
          description: Unique completion identifier
          example: "chatcmpl-123"

        object:
          type: string
          enum: [chat.completion]
          description: Object type
          example: "chat.completion"

        created:
          type: integer
          description: Unix timestamp of creation
          example: 1677652288

        model:
          type: string
          description: Model used for completion
          example: "PaddlePaddle/PaddleOCR-VL"

        choices:
          type: array
          description: Array of completion choices
          items:
            type: object
            required:
              - index
              - message
              - finish_reason
            properties:
              index:
                type: integer
                description: Choice index
                example: 0
              message:
                type: object
                required:
                  - role
                  - content
                properties:
                  role:
                    type: string
                    enum: [assistant]
                    example: "assistant"
                  content:
                    type: string
                    description: Extracted OCR text
                    example: "# Document Title\n\nExtracted text content..."
              finish_reason:
                type: string
                enum: [stop, length]
                description: Reason for completion
                example: "stop"

        usage:
          type: object
          description: Token usage statistics
          required:
            - prompt_tokens
            - completion_tokens
            - total_tokens
          properties:
            prompt_tokens:
              type: integer
              description: Tokens in the prompt (including image)
              example: 512
            completion_tokens:
              type: integer
              description: Tokens generated
              example: 1024
            total_tokens:
              type: integer
              description: Total tokens used
              example: 1536

    # DeepSeek OCR Schemas
    OCRRequest:
      type: object
      description: Request to process an image or PDF document
      properties:
        image_url:
          type: string
          format: uri
          description: URL to an image file (JPEG, PNG, etc.)
          example: "https://example.com/document.png"

        image_base64:
          type: string
          format: byte
          description: Base64-encoded image (with or without data URL prefix)
          example: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..."

        pdf_url:
          type: string
          format: uri
          description: URL to a PDF file
          example: "https://example.com/document.pdf"

        pdf_base64:
          type: string
          format: byte
          description: Base64-encoded PDF document
          example: "JVBERi0xLjQKJeLjz9MKMSAwIG9iago8PC..."

        prompt:
          type: string
          default: "<image>\n<|grounding|>Convert the document to markdown"
          description: |
            OCR instruction prompt. Use grounding tokens for bounding box annotations.

            **Common Prompts:**
            - `<image>\n<|grounding|>Convert to markdown` - Extract with structure
            - `<image>\n<|grounding|>Extract all text` - Plain text extraction
            - `<image>\nTranscribe this document` - Basic transcription
          example: "<image>\n<|grounding|>Convert the document to markdown"

        page_num:
          type: integer
          minimum: 1
          description: |
            PDF page number to extract (1-indexed). Omit to process all pages.

            - **With page_num**: Sequential processing of single page
            - **Without page_num**: Parallel batch processing of all pages
          example: 1

        base_size:
          type: integer
          default: 1024
          enum: [512, 640, 1024, 1280]
          description: Base image size for processing (larger = more detail, slower)
          example: 1024

        image_size:
          type: integer
          default: 640
          description: Target image size (must correspond to base_size)
          example: 640

      oneOf:
        - required: [image_url]
        - required: [image_base64]
        - required: [pdf_url]
        - required: [pdf_base64]

    OCRResponse:
      type: object
      properties:
        text:
          type: string
          description: |
            Extracted text with grounding annotations.

            **Annotation Format:**
            - `<|ref|>type<|/ref|>` - Element type (title, text, sub_title)
            - `<|det|>[[x1, y1, x2, y2]]<|/det|>` - Bounding box coordinates

            Multi-page PDFs use `---PAGE BREAK---` separator.
          example: |
            <|ref|>title<|/ref|><|det|>[[52, 45, 940, 550]]<|/det|>
            # Document Title

            <|ref|>text<|/ref|><|det|>[[52, 720, 760, 975]]<|/det|>
            Body text content here...

        model:
          type: string
          example: "deepseek-ai/DeepSeek-OCR"

        processing_time_ms:
          type: number
          format: float
          description: Total processing time in milliseconds
          example: 18301.39

        pages_processed:
          type: integer
          minimum: 1
          description: Number of pages processed
          example: 1

        throughput_pages_per_sec:
          type: number
          format: float
          description: Throughput in pages per second (only for multi-page processing)
          example: 2.86

        success:
          type: boolean
          example: true

        backend:
          type: string
          enum: [vllm-official-upstream]
          example: "vllm-official-upstream"

        debug:
          type: object
          description: Debug information (may be omitted in production)
          properties:
            request_id:
              type: string
              example: "b6f9f33f"
            final_directory_files:
              type: array
              items:
                type: string
            text_length:
              type: integer
              example: 236
      required:
        - text
        - model
        - processing_time_ms
        - pages_processed
        - success

    # Embedding Schemas
    EmbeddingRequest:
      type: object
      required:
        - texts
      properties:
        texts:
          type: array
          items:
            type: string
          minItems: 1
          maxItems: 100
          description: Array of text strings to embed (max 100 per request)
          example:
            - "First document text"
            - "Second document text"

        model:
          type: string
          deprecated: true
          description: Model parameter is ignored (only Qwen3-Embedding-8B available)
          example: "qwen3-embedding-8b"

    EmbeddingResponse:
      type: object
      properties:
        results:
          type: array
          items:
            $ref: '#/components/schemas/EmbeddingResult'
          description: Array of embedding results (one per input text)

        batch_size:
          type: integer
          description: Number of texts processed
          example: 2

        model:
          type: string
          example: "qwen3-embedding-8b"

        total_request_time_ms:
          type: number
          format: float
          description: Total request processing time
          example: 234.5

        backend:
          type: string
          enum: [TEI]
          example: "TEI"
      required:
        - results
        - batch_size
        - model
        - total_request_time_ms
        - backend

    EmbeddingResult:
      type: object
      properties:
        embedding:
          type: array
          items:
            type: number
            format: float
          minItems: 8192
          maxItems: 8192
          description: Dense embedding vector (8192 dimensions)
          example: [0.123, -0.456, 0.789]

        model:
          type: string
          example: "Qwen/Qwen3-Embedding-8B"

        dimensions:
          type: integer
          example: 8192

        success:
          type: boolean
          example: true

        backend:
          type: string
          enum: [TEI]
          example: "TEI"
      required:
        - embedding
        - model
        - dimensions
        - success

    # Extraction Schemas
    ExtractionRequest:
      type: object
      description: Request to extract structured information from text or images
      properties:
        text:
          type: string
          description: Text to extract information from (for text-only mode)
          example: "Critical bug in the authentication module causing system crashes"

        image_url:
          type: string
          format: uri
          description: URL to an image (for multimodal mode)
          example: "https://example.com/issue-screenshot.png"

        image_base64:
          type: string
          format: byte
          description: Base64-encoded image (for multimodal mode)
          example: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..."

        template:
          type: object
          description: |
            JSON template defining the extraction schema.
            Keys represent fields to extract, values are default/empty values indicating type.

            Supported value types:
            - String: `""` (empty string)
            - Number: `0` (zero)
            - Array: `[]` (empty array)
            - Object: `{}` (empty object)
          example:
            issue_type: ""
            severity: ""
            affected_component: ""
            action_items: []
          additionalProperties: true

        temperature:
          type: number
          format: float
          minimum: 0.0
          maximum: 1.0
          default: 0.0
          description: Sampling temperature (0.0 for deterministic extraction)
          example: 0.0

      required:
        - template
      oneOf:
        - required: [text]
        - required: [image_url]
        - required: [image_base64]

    ExtractionResponse:
      type: object
      properties:
        extracted_data:
          type: object
          description: Extracted structured data matching the template schema
          example:
            issue_type: "Bug"
            severity: "High"
            affected_component: "Login component"
          additionalProperties: true

        model:
          type: string
          description: Model used for extraction
          example: "numind/NuExtract-2.0-8B"

        processing_time_ms:
          type: number
          format: float
          description: Total processing time in milliseconds
          example: 396.27

        template:
          type: object
          description: Original template used for extraction
          additionalProperties: true

        success:
          type: boolean
          description: Whether extraction was successful
          example: true

        backend:
          type: string
          enum: [vLLM]
          description: Backend inference engine
          example: "vLLM"

      required:
        - extracted_data
        - model
        - processing_time_ms
        - template
        - success

    # Common Schemas
    ErrorResponse:
      type: object
      properties:
        error:
          type: string
          description: Human-readable error message
          example: "Processing failed: Invalid input format"

        traceback:
          type: string
          description: Python traceback (only included in OCR errors)
          example: "Traceback (most recent call last)..."

        details:
          type: string
          description: Additional error details (only included in embedding errors)
          example: "Internal Server Error"

        success:
          type: boolean
          description: Always false for errors (only included in OCR errors)
          example: false
      required:
        - error

# ==================== Security ====================

security: []  # No authentication required (Modal handles internal auth)

# ==================== External Documentation ====================

externalDocs:
  description: I2P Modal Services Documentation
  url: https://github.com/your-org/code-ingest/blob/main/docs/MODAL_SERVICES.md
